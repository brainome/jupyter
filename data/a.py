#!/usr/bin/env python3
#
# This code has been produced by a free evaluation version of Brainome(tm).
# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.
# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),
# worldwide, non-sublicensable, and non-transferable limited license to use and modify this
# predictor produced through the input of your data:
# (i) for users accessing the service through a free evaluation account, solely for your
# own non-commercial purposes, including for the purpose of evaluating this service, and
# (ii) for users accessing the service through a paid, commercial use account, for your
# own internal  and commercial purposes.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome v1.006-18-beta.
# Invocation: brainome vehicle.csv vehicle_A.csv.gz vehicle_B.csv.gz
# Total compiler execution time: 0:00:16.61. Finished on: Aug-20-2021 13:11:08.
# This source code requires Python 3.
#
"""

[01;1mPredictor:[0m                        a.py
    Classifier Type:              Random Forest
    System Type:                  4-way classifier
    Training / Validation Split:  50% : 50%
    Accuracy:
      Best-guess accuracy:        25.77%
      Training accuracy:          99.92% (1267/1268 correct)
      Validation Accuracy:        95.11% (1208/1270 correct)
      Combined Model Accuracy:    97.51% (2475/2538 correct)

    Model Capacity (MEC):         65    bits

    Generalization Ratio:         38.96 bits/bit
    Percent of Data Memorized:     6.84%
    Resilience to Noise:          -1.29 dB


    Training Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                 van |   298     0     0     0 
                saab |     0   325     0     0 
                 bus |     0     0   327     0 
                opel |     0     1     0   317 

    Validation Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                 van |   296     0     3     0 
                saab |     3   302     0    21 
                 bus |     0     0   327     0 
                opel |     0    35     0   283 

    Training Accuracy by Class:
               Class |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
               ----- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                 van |   298     0   970     0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%
                saab |   325     1   942     0  100.00%   99.89%   99.69%  100.00%   99.85%   99.69%
                 bus |   327     0   941     0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%
                opel |   317     0   950     1   99.69%  100.00%  100.00%   99.89%   99.84%   99.69%

    Validation Accuracy by Class:
               Class |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
               ----- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                 van |   296     3   968     3   99.00%   99.69%   99.00%   99.69%   99.00%   98.01%
                saab |   302    35   909    24   92.64%   96.29%   89.61%   97.43%   91.10%   83.66%
                 bus |   327     3   940     0  100.00%   99.68%   99.09%  100.00%   99.54%   99.09%
                opel |   283    21   931    35   88.99%   97.79%   93.09%   96.38%   91.00%   83.48%


    Attribute Ranking:
                                      Feature | Relative Importance
                                ELONGATEDNESS :   0.1984
                      MAX.LENGTH_ASPECT_RATIO :   0.1578
                        SCALED_VARIANCE_MINOR :   0.1473
                       PR.AXIS_RECTANGULARITY :   0.0860
                                HOLLOWS_RATIO :   0.0501
                    MAX.LENGTH_RECTANGULARITY :   0.0423
                                  CIRCULARITY :   0.0419
                         SKEWNESS_ABOUT_MAJOR :   0.0390
                                  COMPACTNESS :   0.0382
                         PR.AXIS_ASPECT_RATIO :   0.0338
                        SCALED_VARIANCE_MAJOR :   0.0301
                                SCATTER_RATIO :   0.0222
                         SKEWNESS_ABOUT_MINOR :   0.0217
                    SCALED_RADIUS_OF_GYRATION :   0.0216
                         DISTANCE_CIRCULARITY :   0.0214
                         KURTOSIS_ABOUT_MAJOR :   0.0198
                         KURTOSIS_ABOUT_MINOR :   0.0178
                                 RADIUS_RATIO :   0.0107
         

"""

import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler
import json
from io import StringIO
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.")
    sys.exit(1)
try:
    from scipy.sparse import coo_matrix
    report_cmat = True
except:
    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.")
    report_cmat = False
try:
    import multiprocessing
    var_dict = {}
    default_to_serial = False
except:
    default_to_serial = True

IOBUF = 100000000
sys.setrecursionlimit(1000000)
TRAINFILE = ['vehicle.csv', 'vehicle_A.csv.gz', 'vehicle_B.csv.gz']
mapping = {'van': 0, 'saab': 1, 'bus': 2, 'opel': 3}
ignorelabels = []
ignorecolumns = []
target = '' 
target_column = 18
important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
ignore_idxs = []
classifier_type = 'RF'
num_attr = 18
n_classes = 4
model_cap = 65
logits_dict = {0: array([0.0, 0.0, 0.0, 0.0, -0.527658761, 0.0, 0.0, -0.454285741, 1.15137935, 0.0, 0.0, -0.366923094, 0.0, -0.0, 1.1133405, 0.0635999963, -0.506411731, 1.54603696, 0.0935294107]), 1: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61153847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.954000056, -0.190799996, 0.907046974, 0.0459537581, 0.0512903221, -0.442577302, 0.622173905, -0.402289182, -0.164482772, -0.517343283, 0.619830489, -0.220676169, 0.83506161, -0.482359529]), 2: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.10076928, 0.0, 0.0, -0.449999988, 1.46405947, 0.0, 0.0, -0.522045076, -0.22714287, -0.49835822, 1.05442107, 1.38532782, -0.0836842135, 0.0935294107, -0.517809749, 1.10076928, -0.454285741]), 3: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71275866, -0.517343283, 0.0, 0.0, -0.0691304356, 1.07081628, -0.278071761, 0.333233535, -0.476164609, -0.0836842135, 1.04485714, -0.356938779, 0.484935343, 1.39580154, 0.654705882, -0.369107127]), 4: array([0.0, 0.0, 0.0, 0.0, -0.450987279, 0.0, 0.0, -0.373382092, 0.545355141, 0.0, 0.0, -0.382371217, 0.0, 0.661333859, -0.359719902, 0.426365048, -0.359648108, -0.226334423, 0.499161214]), 5: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.420404345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.135388926, 0.228154361, -0.162306041, 0.854236305, -0.386894673, 0.385940611, -0.434035122, -0.110921726, -0.212746248, 0.446741521, 0.161427438, 0.719339073]), 6: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.446499854, 0.0953879729, 0.0, 0.612315297, -0.376912087, 0.0, 0.0, 0.0, -0.251218796, 0.408538461, 0.602969766, -0.0648785755, -0.218482137, -0.455547154, 0.50967294, -0.337772816]), 7: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.456571549, 0.0, 0.0, 0.0, 0.0, 0.483340055, -0.45555979, 0.318355918, -0.16381526, 0.560669303, -0.23068656, -0.311101258, 1.05982721, 0.661088467, -0.462977856, 0.184038147, 0.561221957, 0.38116011, -0.104522772]), 8: array([0.0, -0.422793448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.440574169, 0.257446468, -0.403998733, 0.114253588, -0.321604431, 0.183910832, -0.0582081787, 0.46947825]), 9: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.121145032, -0.484827876, -0.414808959, 0.113007024, 0.418069333, -0.298835039, 0.00844907947, 0.231419265, -0.0576743484, -0.467255026, 0.411510348, 1.44702458, -0.362956226, 0.575232685]), 10: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.422446817, 0.0, -0.422640353, -0.0356228687, 0.0, 0.0, 0.54263258, -0.0796824619, 0.439809889, -0.347722203, 0.440318853, -0.391667426]), 11: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.32915771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.560457706, -0.250259757, -0.201926455, -0.476111591, 0.709573448, 0.14081338, 0.172066763, 0.64768374, 0.332449496, -0.388226062, -0.0833833814, 0.255069137, -0.362387538, 0.468266606]), 12: array([0.0, -0.406588972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.511411846, -0.175960809, 0.205207989, -0.414816439, 0.236491874, -0.33414948, -0.406484008, 0.461213022]), 13: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.495895416, 0.0, -0.0400959924, 0.0, 0.572688699, 0.0, 0.0, -0.0105910776, 0.203506961, 0.683986783, 0.275349379, 0.189575702, -0.35141468, -0.402233601, -0.784008324, 0.186712831, -0.416900039]), 14: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.405292958, 0.312985599, 0.0, 0.0, 0.0, 0.0, -0.453864485, -0.120106116, -0.128850698, 0.384620398, 0.397271395, -0.244580686, -0.298390299, 0.277194977]), 15: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.614400387, 0.0, 0.0, 0.117656581, 0.680230677, -0.431128025, 0.0675023347, -0.191423953, 0.0779890418, 0.396526009, -0.391484857, -0.362867117, 0.291515201, 0.385112762, -0.554852903, 0.611724973, -0.344619572]), 16: array([0.0, 0.0, 0.0, 0.0, -0.396804214, 0.0, 0.0, -0.253977239, 0.261817336, 0.0, 0.0, -0.281154305, 0.0, 0.39296937, -0.0885539949, -0.429768473, 0.124491908, -0.0340594873, 0.377501309]), 17: array([0.0, 0.0, -0.454614133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.431168824, 0.100642987, 0.193685129, 0.0102595026, -0.190045565, -0.600920558, 0.447618306, -0.160502926]), 18: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.259149998, -0.391738206, 0.0, 0.0, 0.0, -0.243498549, -0.354113728, 0.161518544, -0.522179365, 0.226889923, 0.151613697, 0.418224752]), 19: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0939516202, 0.618662179, 0.302902102, -0.349597752, 0.511250854, -0.078120403, 0.999192357, 0.0669579729, 0.357441664, -0.030617265, -0.173390716, 0.0475821309, 0.511970997, -0.247385591, -0.407927573, 0.0780116469]), 20: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.390762538, 0.00565895671, 0.0, 0.0, 0.0, 0.454049915, 0.446151853, -0.347857296, 0.365832835, -0.304497302, -0.184235394, 0.35245356]), 21: array([0.0, 0.0, -0.444414884, 0.0, 0.0, 0.0, 0.0, 0.0, -0.292430133, -0.4247199, 0.0105813509, 0.13169983, -0.0277275704, -0.329036415, 0.504318953]), 22: array([0.0, 0.0, 0.0, -0.303582668, 0.382194668, 0.0, 0.0, 0.0, 0.129192695, 0.0, 0.0, -0.0245333612, -0.441494226, -0.670718074, 0.265967667, 0.0735772103, -0.360381693]), 23: array([0.0, 0.0, 0.0, 0.0, 0.53908658, -0.43735379, -0.0201850832, 0.0, 0.0, 0.183158234, -0.409270138, 0.673471391, 0.0283151101]), 24: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.37320751, 0.020134287, 0.0, 0.0, 0.0, 0.0, 0.358893752, 0.0641349629, 0.176729247, -0.288320214, 0.520678341, -0.171057343, -0.203836188, 0.323724955]), 25: array([0.0, 0.0, 0.0, 0.372600764, 0.0, 0.0, 0.0, 0.160434932, 0.0, 0.0, 0.0, 0.0, 0.0, -0.177738637, -0.473381609, -0.317964971, 0.37501803, -0.341057241, 0.32284683, -0.535653174, 0.148360968, 0.321448326, 0.0120225744]), 26: array([0.0, 0.0, 0.0, -0.454063654, 0.0, 0.0, 0.0, -0.401715904, 0.0, 0.0, 0.0, -0.462321937, 0.0, 0.0297649112, 0.338238537, -0.000342219806, -0.341209084, -0.224097177, 0.421456516, -0.00275606452, 0.355588883]), 27: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18823798, -0.164576516, 0.458984405, 0.0122816619, 0.370569736, -0.187291875, 0.462305158, -0.0291613284, 0.566999078, 0.00434966432, -0.0700819716, -0.379497916, 0.224056527, -0.415128201, 0.632317662, -0.100641347]), 28: array([0.0, 0.0, 0.0, -0.0142560704, -0.359711617, 0.0, 0.0, 0.0, 0.0, 0.00102119881, 0.0, -0.0345295444, 0.313023835, -0.386951953, 0.00420708256, 0.0922580287, 0.385210097]), 29: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.458181381, -0.225133911, 0.0, 0.0, 0.0, 0.424616605, -0.0573036261, 0.00554722967, 0.256882489, 0.516857922, 0.0273072142, -0.101584978, -0.366552681, -0.177246258, 0.476006955]), 30: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.448342592, 0.0, 0.0, 0.0, -0.245880142, 0.235042661, -0.390057355, -0.112593457, -0.278244644, 0.225396857, -0.246069714, 0.35882017, 0.0716657862, -0.325490624]), 31: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.503642857, 0.0987395495, 0.0, -0.34722203, -0.315992981, 0.0, -0.0208830684, 0.141439989, 0.0153906764, -0.450854212, -0.235843733, 0.439286053, 0.309469491, -0.0193188079]), 32: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.131930336, 0.0, 0.0, -0.450591087, 0.0, 0.0, 0.0, 0.0247061495, -0.0940941945, 0.333787531, 0.0656355843, -0.340044171, -0.160461292, 0.31712833, -0.110626213, -0.346535355]), 33: array([0.0, 0.0, 0.0, -0.0269579105, -0.404631644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.151521668, 0.349202245, -0.137795672, 0.0155563867, -0.367818415, 0.257898211, -0.00437145727, 0.417301506]), 34: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040021088, -0.128124684, 0.312067449, -0.38319096, 0.0, 0.0126424795, -0.305215269, 0.0121759679, -0.445610315, 0.0482175648, 0.30317536]), 35: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0543839224, 0.370604426, -0.347641647, 0.154769778, 0.439344883, 0.0335361548, -0.28312099, 0.227522165, 0.064032279, -0.26304996, 0.354391903, -0.00677065458, 0.0174267795, -0.313702404, 0.326029718, -0.0304361396]), 36: array([0.0, 0.0, 0.0, 0.381998003, -0.270227045, 0.0, 0.0, 0.193288833, 0.0, 0.0, 0.0, 0.167501435, -0.371270299, 0.308715224, -0.205771685, -0.442302525, 0.00314107467]), 37: array([0.0, 0.0, -0.383992672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.182773605, 0.301440388, 0.0268677734, -0.390255392, 0.555540144, -0.30479005, -0.0611640438, 0.0493037701]), 38: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.428382307, -0.0785168111, 0.0, 0.0, -0.32527861, 0.0, 0.394888192, -0.0618087202, -0.00202623289, -0.363537848, -0.0684209764, 0.29497838, 0.296116859, -0.267847002]), 39: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15354614, -0.0704495907, -0.387207478, -0.0740511268, 0.190490276, -0.249089822, 0.0880026892, 0.437687904, -0.22478275, 0.348623782, -0.382142961, -0.0318307281, 0.460704178, 0.04201198, -0.372677118, 0.125135139]), 40: array([0.0, -0.308808565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023300536, 0.335233033, -0.0151895806, 0.341641217, -0.34921509, 0.0420022346]), 41: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.481607258, 0.0, 0.0, 0.0, -0.35395506, 0.032593295, -0.0290404446, -0.520602703, 0.104205132, -0.0899656042, 0.0208243784, 0.396449208, 0.350326568, -0.161132663, 0.323903829, -0.0602443293, -0.39593792, 0.0659290031]), 42: array([0.0, 0.0, 0.0, -0.347675472, 0.0, 0.0, 0.0, -0.161194593, 0.216555372, 0.0, -0.193389311, 0.0, 0.0, -0.127220392, 0.286127448, -0.335492522, 0.0825557411, 0.204972103, -0.26879555]), 43: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.368107378, 0.0, 0.0, 0.0, 0.0, 0.0102224341, 0.0953240469, -0.0114880055, 0.394998193, 0.276758879, -0.412193447, 0.498204708, 0.0998975486, 0.40547207, -0.00634610327]), 44: array([0.0, -0.307969242, 0.0, 0.0, 0.0, -0.159585729, 0.0, 0.0, 0.0, 0.356118023, 0.0816178918, 0.124356642, -0.270582974, 0.187255979, -0.199048698]), 45: array([0.0, 0.0, 0.0, 0.0, -0.369204819, 0.0, 0.0, -0.268124849, 0.0, -0.114296794, 0.0, 0.0, 0.0, 0.153930962, -0.260139227, 0.436931491, 0.0523435585, 0.0759191662, -0.353908628, -0.0308449864, 0.106477976]), 46: array([0.0, 0.0, 0.0, 0.183102891, 0.0, 0.0, 0.0, 0.122362442, 0.0, -0.279365152, 0.0, 0.334081769, 0.0114249606, -0.254376084, 0.0121874195, 0.20607695, -0.213628858]), 47: array([0.0, 0.319928288, 0.0, 0.0, 0.0, 0.0227656402, -0.365038782, 0.0, 0.0, -0.19062078, 0.262367845, -0.40222615, 0.00864581112]), 48: array([0.0, 0.0, 0.0, 0.0, 0.323461413, 0.0, 0.0, 0.0, 0.0, -0.344155192, -0.0257547647, 0.0, 0.0, -0.0116196945, 0.290827453, -0.246403098, 0.10551095, -0.277848542, 0.112332605, 0.299450755, -0.15915744]), 49: array([0.0, 0.0, 0.0, 0.0, -0.398156315, 0.0, 0.0, 0.0, 0.0, -0.243253395, 0.225011528, 0.0, -0.0570424832, 0.0264298096, -0.192104056, 0.423182815, 0.0489765964, -0.413355023, -0.0476367548]), 50: array([0.0, 0.0, 0.0, 0.0, 0.1303249, 0.0, 0.0, -0.308714181, -0.0079751797, 0.0, -0.150616169, -0.245481089, 0.0775245503, -0.0927011743, 0.233929455]), 51: array([0.0, 0.0, 0.0, 0.0, 0.370860547, 0.0, 0.0, 0.0, 0.0, 0.192307934, -0.215532765, 0.0, 0.0690418109, -0.0256186221, 0.0842698738, -0.400669068, -0.0372475199, 0.412195832, 0.0632418469]), 52: array([0.0, -0.247179791, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0107864393, 0.0, -0.0376258083, 0.203632951, 0.048632469, -0.283926606, 0.0761639401, 0.278903514]), 53: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.219106019, 0.0, -0.332686335, -0.071163699, 0.0, -0.338533461, 0.0, 0.136913091, -0.281685829, 0.158047259, -0.238762677, 0.413465917, 0.0197629705, 0.0150648551, -0.271878302]), 54: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0197291132, -0.25191009, 0.0550437681, 0.265385389, -0.0306297336, 0.225730419, 0.0, 0.0542959273, -0.266171426, -0.00220972067]), 55: array([0.0, 0.0, 0.0, 0.337100685, 0.0056584361, 0.0, 0.0, 0.0, 0.0, -0.313787907, 0.104687899, -0.226381317, 0.263501316, -0.060738083, 0.0318988599]), 56: array([0.0, 0.0, 0.0, -0.0688987747, 0.295155674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0776035115, -0.293151289, 0.251882464, -0.179366693, -0.121539302, 0.23800464, -0.250447959, -0.0127801588]), 57: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.163086697, 0.0758770183, 0.0, 0.0, 0.0, 0.0159594975, -0.204124868, 0.0551987998, 0.393725216, 0.145043373, -0.344059944, 0.102254264, 0.451997668, -0.0630214214, -0.377356857, 0.139649704, -0.0707770661]), 58: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.0365297757, -0.227663457, -0.23871161, 0.0161164589, 0.0, -0.146898255, 0.0578300804, 0.238955066]), 59: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.390201271, 0.0, -0.294056475, -0.121658482, -0.444982857, 0.0, 0.0, -0.00314006116, 0.457775146, 0.358975649, -0.100320004, -0.0425727554, 0.309995085, -0.157355323, 0.0700799376]), 60: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0444645062, -0.172000229, 0.286645353, 0.0438128114, -0.306746632, 0.0, 0.237556443, 0.0, 0.132389307, -0.207471952, -0.174758941, 0.0810090974]), 61: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00863348786, 0.354865998, 0.220854506, -0.0788844973, 0.0, 0.0402128734, 0.00584716955, 0.185351893, -0.41394034, 0.238794997, -0.395275742, -0.124022126]), 62: array([0.0, 0.0, 0.0, 0.0, 0.0, -0.0904514194, 0.254670739, 0.0199893173, -0.256837189, -0.201398939, 0.0, 0.239701048, -0.0267179441]), 63: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0578056425, -0.362769485, -0.21818693, 0.0833013803, 0.340246171, -0.0426837727, -0.00334178726, -0.178525195, 0.416195124, -0.235311225]), 64: array([0.0, -0.208008096, 0.0, 0.199454978, 0.0, 0.0, 0.0, -0.00938521978, -0.256120086, 0.15168786, -0.146081403]), 65: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.302832723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00484286528, 0.367295802, -0.519540906, 0.0433419384, -0.189909831, 0.310059845, -0.263066947, 0.285827935, -0.318071663, 0.0022482404, -0.452759296, -0.0236747861, -0.112280659, 0.244511902]), 66: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0980759338, -0.242498875, 0.00140930421, 0.200516075, -0.159017026, 0.0506544597, 0.209840387]), 67: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.394858748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01875869, -0.379959196, -0.0690413788, 0.364839435, 0.267100543, -0.350413412, 0.219364256, -0.277608871, 0.316515058, 0.00333963335, 0.453906238, 0.0273569301, 0.115317836, -0.241049767])}
right_children_dict = {0: array([1, 3, 5, 7, -1, 9, 11, -1, -1, 13, 15, -1, 17, -1, -1, -1, -1, -1, -1]), 1: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 2: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, -1, -1, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 3: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, -1, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 4: array([1, 3, 5, 7, -1, 9, 11, -1, -1, 13, 15, -1, 17, -1, -1, -1, -1, -1, -1]), 5: array([1, 3, 5, 7, 9, 11, -1, 13, 15, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 6: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, -1, 17, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1]), 7: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, -1, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 8: array([1, -1, 3, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1]), 9: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 10: array([1, 3, 5, 7, 9, -1, 11, -1, -1, 13, 15, -1, -1, -1, -1, -1, -1]), 11: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 12: array([1, -1, 3, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1]), 13: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, -1, 19, -1, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 14: array([1, 3, 5, 7, 9, -1, -1, 11, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 15: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, -1, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 16: array([1, 3, 5, 7, -1, 9, 11, -1, -1, 13, 15, -1, 17, -1, -1, -1, -1, -1, -1]), 17: array([1, 3, -1, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1]), 18: array([1, 3, 5, 7, 9, -1, -1, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1]), 19: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 20: array([1, 3, 5, 7, 9, -1, -1, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1]), 21: array([1, 3, -1, 5, 7, 9, 11, 13, -1, -1, -1, -1, -1, -1, -1]), 22: array([1, 3, 5, -1, -1, 7, 9, 11, -1, 13, 15, -1, -1, -1, -1, -1, -1]), 23: array([1, 3, 5, 7, -1, -1, -1, 9, 11, -1, -1, -1, -1]), 24: array([1, 3, 5, 7, 9, -1, -1, 11, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 25: array([1, 3, 5, -1, 7, 9, 11, -1, 13, 15, 17, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 26: array([1, 3, 5, -1, 7, 9, 11, -1, 13, 15, 17, -1, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 27: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 28: array([1, 3, 5, -1, -1, 7, 9, 11, 13, -1, 15, -1, -1, -1, -1, -1, -1]), 29: array([1, 3, 5, 7, 9, 11, 13, 15, -1, -1, 17, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 30: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 31: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, -1, 19, -1, -1, 21, -1, -1, -1, -1, -1, -1, -1, -1]), 32: array([1, 3, 5, 7, 9, -1, 11, 13, -1, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 33: array([1, 3, 5, -1, -1, 7, 9, 11, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 34: array([1, 3, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, 17, -1, -1, -1, -1, -1, -1]), 35: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 36: array([1, 3, 5, -1, -1, 7, 9, -1, 11, 13, 15, -1, -1, -1, -1, -1, -1]), 37: array([1, 3, -1, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1]), 38: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, -1, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 39: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 40: array([1, -1, 3, 5, 7, 9, 11, -1, -1, -1, -1, -1, -1]), 41: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, -1, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 42: array([1, 3, 5, -1, 7, 9, 11, -1, -1, 13, -1, 15, 17, -1, -1, -1, -1, -1, -1]), 43: array([1, 3, 5, 7, 9, 11, -1, 13, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 44: array([1, -1, 3, 5, 7, -1, 9, 11, 13, -1, -1, -1, -1, -1, -1]), 45: array([1, 3, 5, 7, -1, 9, 11, -1, 13, -1, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 46: array([1, 3, 5, -1, 7, 9, 11, -1, 13, -1, 15, -1, -1, -1, -1, -1, -1]), 47: array([1, -1, 3, 5, 7, -1, -1, 9, 11, -1, -1, -1, -1]), 48: array([1, 3, 5, 7, -1, 9, 11, 13, 15, -1, -1, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 49: array([1, 3, 5, 7, -1, 9, 11, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1, -1]), 50: array([1, 3, 5, 7, -1, 9, 11, -1, -1, 13, -1, -1, -1, -1, -1]), 51: array([1, 3, 5, 7, -1, 9, 11, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1, -1]), 52: array([1, -1, 3, 5, 7, 9, 11, -1, 13, -1, -1, -1, -1, -1, -1]), 53: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, -1, -1, 19, -1, 21, -1, -1, -1, -1, -1, -1, -1, -1]), 54: array([1, 3, 5, 7, 9, 11, 13, -1, -1, -1, -1, -1, -1, 15, -1, -1, -1]), 55: array([1, 3, 5, -1, -1, 7, 9, 11, 13, -1, -1, -1, -1, -1, -1]), 56: array([1, 3, 5, -1, -1, 7, 9, 11, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 57: array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, -1, -1, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 58: array([1, 3, 5, 7, 9, -1, -1, -1, -1, 11, -1, -1, -1]), 59: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, -1, -1, -1, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1]), 60: array([1, 3, 5, 7, 9, 11, 13, -1, -1, -1, -1, -1, 15, -1, 17, -1, -1, -1, -1]), 61: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, -1, -1, -1, 19, -1, -1, -1, -1, -1, -1, -1]), 62: array([1, 3, 5, 7, 9, -1, -1, -1, -1, -1, 11, -1, -1]), 63: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 64: array([1, -1, 3, -1, 5, 7, 9, -1, -1, -1, -1]), 65: array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 66: array([1, 3, 5, 7, 9, 11, -1, -1, -1, -1, -1, -1, -1]), 67: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])}
split_feats_dict = {0: array([7, 10, 5, 1, 0, 11, 17, 0, 0, 9, 11, 0, 15, 0, 0, 0, 0, 0, 0]), 1: array([11, 9, 5, 11, 13, 0, 17, 9, 17, 0, 5, 4, 13, 5, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2: array([5, 11, 17, 4, 13, 1, 4, 11, 0, 4, 14, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 3: array([10, 9, 5, 0, 14, 0, 0, 4, 15, 16, 4, 0, 0, 9, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 4: array([7, 10, 5, 1, 0, 8, 16, 0, 0, 7, 4, 0, 9, 0, 0, 0, 0, 0, 0]), 5: array([0, 9, 11, 2, 13, 13, 0, 15, 0, 10, 12, 17, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 6: array([5, 11, 17, 4, 4, 1, 4, 0, 0, 13, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]), 7: array([11, 4, 5, 8, 9, 0, 0, 0, 9, 3, 0, 6, 13, 11, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 8: array([7, 0, 5, 6, 0, 0, 9, 14, 13, 0, 0, 0, 0, 0, 0, 0, 0]), 9: array([14, 2, 16, 9, 10, 10, 4, 14, 13, 17, 2, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 10: array([5, 11, 5, 12, 2, 0, 0, 0, 0, 14, 7, 0, 0, 0, 0, 0, 0]), 11: array([2, 15, 17, 16, 2, 13, 13, 0, 17, 0, 3, 11, 0, 14, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 12: array([7, 0, 5, 7, 4, 0, 15, 3, 17, 0, 0, 0, 0, 0, 0, 0, 0]), 13: array([16, 17, 4, 14, 3, 3, 4, 12, 0, 4, 0, 6, 0, 3, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 14: array([5, 4, 5, 12, 5, 0, 0, 4, 13, 15, 4, 0, 0, 0, 0, 0, 0, 0, 0]), 15: array([15, 13, 17, 17, 17, 4, 4, 11, 12, 2, 4, 6, 0, 9, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 16: array([7, 10, 9, 1, 0, 12, 16, 0, 0, 15, 7, 0, 5, 0, 0, 0, 0, 0, 0]), 17: array([13, 4, 0, 7, 9, 17, 16, 15, 14, 0, 0, 0, 0, 0, 0, 0, 0]), 18: array([5, 0, 17, 2, 5, 0, 0, 11, 4, 3, 0, 0, 0, 0, 0, 0, 0]), 19: array([0, 11, 7, 10, 14, 13, 15, 1, 12, 9, 15, 17, 11, 4, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 20: array([10, 9, 13, 6, 7, 0, 0, 7, 12, 9, 0, 0, 0, 0, 0, 0, 0]), 21: array([12, 12, 0, 15, 9, 2, 9, 15, 0, 0, 0, 0, 0, 0, 0]), 22: array([17, 1, 4, 0, 0, 13, 2, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0]), 23: array([0, 14, 9, 2, 0, 0, 0, 16, 10, 0, 0, 0, 0]), 24: array([10, 4, 13, 4, 12, 0, 0, 6, 6, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0]), 25: array([12, 13, 1, 0, 4, 11, 12, 0, 16, 7, 15, 11, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 26: array([9, 4, 13, 0, 16, 16, 10, 0, 4, 2, 13, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0]), 27: array([4, 14, 15, 9, 2, 17, 3, 10, 0, 13, 2, 12, 13, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 28: array([7, 10, 5, 0, 0, 8, 17, 9, 4, 0, 13, 0, 0, 0, 0, 0, 0]), 29: array([4, 4, 5, 1, 3, 9, 14, 12, 0, 0, 3, 13, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 30: array([14, 9, 5, 16, 2, 0, 16, 0, 4, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 31: array([10, 1, 17, 12, 2, 4, 10, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 32: array([10, 4, 3, 3, 17, 0, 4, 9, 0, 15, 9, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 33: array([4, 14, 14, 0, 0, 5, 15, 3, 3, 14, 9, 0, 0, 0, 0, 0, 0, 0, 0]), 34: array([4, 13, 5, 10, 9, 2, 2, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0]), 35: array([0, 11, 16, 12, 9, 11, 0, 1, 12, 5, 4, 14, 10, 15, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 36: array([6, 7, 9, 0, 0, 3, 12, 0, 12, 6, 9, 0, 0, 0, 0, 0, 0]), 37: array([12, 3, 0, 2, 3, 3, 0, 9, 12, 0, 0, 0, 0, 0, 0, 0, 0]), 38: array([16, 9, 13, 4, 13, 4, 14, 0, 0, 0, 5, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0]), 39: array([14, 15, 11, 4, 4, 12, 11, 15, 15, 17, 14, 17, 14, 15, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 40: array([7, 0, 5, 11, 9, 9, 13, 0, 0, 0, 0, 0, 0]), 41: array([17, 13, 16, 15, 0, 16, 4, 6, 11, 9, 7, 0, 6, 16, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 42: array([9, 16, 2, 0, 4, 14, 13, 0, 0, 2, 0, 16, 7, 0, 0, 0, 0, 0, 0]), 43: array([0, 11, 13, 11, 12, 17, 0, 11, 12, 17, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 44: array([0, 0, 3, 1, 5, 0, 11, 12, 10, 0, 0, 0, 0, 0, 0]), 45: array([10, 9, 2, 0, 0, 16, 4, 0, 4, 0, 0, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0]), 46: array([16, 17, 13, 0, 0, 4, 14, 0, 10, 0, 10, 0, 0, 0, 0, 0, 0]), 47: array([16, 0, 2, 9, 10, 0, 0, 3, 11, 0, 0, 0, 0]), 48: array([11, 9, 13, 12, 0, 4, 17, 9, 14, 0, 0, 6, 11, 0, 0, 0, 0, 0, 0, 0, 0]), 49: array([3, 1, 16, 2, 0, 17, 17, 11, 6, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]), 50: array([4, 13, 2, 6, 0, 15, 13, 0, 0, 11, 0, 0, 0, 0, 0]), 51: array([3, 1, 16, 12, 0, 17, 17, 1, 14, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]), 52: array([7, 0, 9, 11, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0]), 53: array([3, 2, 17, 2, 4, 12, 16, 12, 0, 9, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0]), 54: array([5, 3, 2, 3, 13, 2, 4, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0]), 55: array([0, 6, 13, 0, 0, 16, 14, 3, 10, 0, 0, 0, 0, 0, 0]), 56: array([6, 10, 4, 0, 0, 0, 10, 3, 3, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0]), 57: array([2, 2, 10, 2, 5, 10, 17, 2, 16, 15, 0, 0, 14, 14, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 58: array([14, 9, 15, 16, 5, 0, 0, 0, 0, 15, 0, 0, 0]), 59: array([2, 2, 10, 4, 5, 14, 17, 11, 0, 6, 0, 0, 0, 5, 16, 0, 0, 0, 0, 0, 0, 0, 0]), 60: array([8, 10, 9, 14, 0, 2, 12, 0, 0, 0, 0, 0, 15, 0, 13, 0, 0, 0, 0]), 61: array([10, 10, 14, 11, 9, 4, 0, 12, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0]), 62: array([16, 13, 13, 10, 9, 0, 0, 0, 0, 0, 2, 0, 0]), 63: array([10, 10, 14, 11, 6, 4, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 64: array([0, 0, 3, 0, 5, 11, 3, 0, 0, 0, 0]), 65: array([9, 2, 16, 5, 17, 15, 17, 3, 14, 0, 12, 10, 4, 10, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 66: array([4, 9, 2, 16, 17, 11, 0, 0, 0, 0, 0, 0, 0]), 67: array([9, 0, 16, 10, 12, 15, 17, 8, 0, 2, 17, 10, 4, 10, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
split_vals_dict = {0: array([42.5, 177.5, 8.5, 45.0, 0.0, 294.5, 189.5, 0.0, 0.0, 127.5, 309.5, 0.0, 23.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: array([372.5, 133.5, 7.5, 262.0, 61.5, 94.5, 196.5, 118.5, 195.5, 0.0, 5.5, 64.5, 68.5, 8.5, 721.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 2: array([7.5, 316.0, 189.5, 64.5, 68.5, 41.0, 71.5, 298.5, 0.0, 63.0, 11.5, 0.0, 0.0, 83.5, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 3: array([180.5, 135.5, 6.5, 82.5, 13.5, 89.5, 106.5, 53.5, 19.5, 201.5, 57.5, 0.0, 0.0, 173.5, 195.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 4: array([42.5, 177.5, 7.5, 45.0, 0.0, 17.5, 181.5, 0.0, 0.0, 58.5, 52.5, 0.0, 128.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 5: array([103.5, 134.5, 723.0, 79.5, 64.5, 67.5, 0.0, 12.5, 88.5, 161.5, 164.5, 204.5, 99.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 6: array([7.5, 305.5, 191.5, 63.5, 66.5, 40.5, 71.5, 0.0, 0.0, 71.5, 0.0, 0.0, 88.5, 70.5, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 7: array([387.5, 62.5, 7.5, 17.5, 126.5, 90.5, 103.5, 81.5, 138.5, 180.5, 0.0, 161.5, 65.5, 588.0, 67.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 8: array([41.5, 0.0, 8.5, 139.5, 85.5, 82.5, 146.5, 8.0, 63.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 9: array([15.5, 76.5, 196.5, 134.5, 233.0, 166.5, 61.5, 0.5, 61.5, 198.5, 99.0, 16.5, 47.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 10: array([8.5, 305.5, 16.0, 172.0, 75.5, 0.0, 86.5, 0.0, 0.0, 10.5, 36.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 11: array([72.5, 26.5, 198.5, 178.5, 65.0, 76.5, 67.5, 82.5, 203.5, 0.0, 157.0, 705.5, 81.5, 6.5, 233.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 12: array([41.5, 0.0, 7.5, 50.5, 61.5, 88.5, 23.5, 150.0, 191.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 13: array([196.5, 204.5, 61.5, 16.5, 214.0, 173.5, 63.5, 212.5, 0.0, 63.5, 0.0, 141.5, 0.0, 189.5, 64.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 14: array([8.5, 63.5, 16.0, 170.5, 7.5, 0.0, 0.0, 60.5, 80.5, 23.5, 70.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 15: array([17.5, 66.5, 203.5, 204.5, 198.5, 70.5, 61.5, 512.5, 159.5, 76.5, 59.5, 125.0, 0.0, 122.5, 63.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 16: array([42.5, 177.5, 143.5, 45.0, 0.0, 138.5, 181.5, 0.0, 0.0, 12.5, 51.5, 0.0, 6.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 17: array([86.5, 66.5, 0.0, 30.5, 163.5, 198.5, 190.5, 19.5, 3.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 18: array([8.5, 94.5, 191.5, 76.5, 7.5, 0.0, 0.0, 305.5, 68.0, 170.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 19: array([88.5, 337.5, 46.5, 160.5, 8.5, 66.5, 1.5, 37.5, 137.0, 133.5, 15.0, 203.5, 588.0, 56.5, 33.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 20: array([179.5, 136.5, 76.5, 120.5, 48.5, 0.0, 0.0, 58.5, 116.5, 146.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 21: array([240.5, 219.5, 0.0, 1.5, 173.5, 81.5, 134.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 22: array([184.5, 41.5, 60.5, 0.0, 0.0, 80.5, 79.5, 83.5, 0.0, 65.0, 6.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 23: array([109.5, 19.5, 168.0, 65.5, 0.0, 0.0, 0.0, 179.5, 147.5, 0.0, 0.0, 0.0, 0.0]), 24: array([179.5, 57.5, 76.5, 52.5, 138.5, 0.0, 0.0, 139.5, 120.5, 153.5, 143.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 25: array([122.0, 61.5, 37.5, 0.0, 55.5, 219.5, 144.5, 0.0, 196.5, 58.5, 2.0, 350.5, 165.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 26: array([135.5, 62.5, 74.5, 0.0, 191.5, 195.5, 165.5, 0.0, 65.0, 75.5, 65.5, 0.0, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 27: array([59.5, 8.5, 26.5, 169.5, 72.5, 196.5, 208.5, 154.5, 104.0, 64.5, 103.5, 142.5, 70.5, 97.0, 108.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 28: array([42.5, 177.5, 8.5, 0.0, 0.0, 18.5, 191.5, 138.5, 61.5, 0.0, 65.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 29: array([66.5, 65.5, 11.5, 54.5, 173.5, 170.5, 4.5, 217.5, 0.0, 0.0, 209.5, 69.5, 30.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 30: array([9.5, 135.5, 6.5, 191.5, 75.5, 93.0, 197.5, 0.0, 62.5, 297.5, 7.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 31: array([226.5, 55.5, 198.5, 215.5, 102.0, 71.5, 229.5, 48.5, 102.5, 0.0, 0.0, 8.5, 0.0, 0.0, 102.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 32: array([173.5, 58.5, 149.5, 142.5, 191.5, 0.0, 70.5, 136.5, 0.0, 4.5, 128.5, 185.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 33: array([52.5, 3.5, 12.5, 0.0, 0.0, 5.5, 4.5, 119.5, 156.5, 17.5, 143.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 34: array([60.5, 80.5, 8.5, 214.5, 141.5, 67.0, 83.5, 83.5, 0.0, 0.0, 0.0, 0.0, 71.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 35: array([88.5, 339.5, 196.5, 144.5, 145.5, 528.5, 91.5, 37.5, 161.5, 8.5, 62.5, 3.5, 212.5, 3.5, 8.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 36: array([119.5, 58.5, 129.5, 0.0, 0.0, 113.5, 143.5, 0.0, 116.0, 146.5, 136.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 37: array([240.5, 131.5, 0.0, 57.5, 137.5, 116.5, 82.5, 136.5, 168.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 38: array([191.5, 138.5, 67.5, 63.5, 74.5, 63.5, 10.0, 0.0, 0.0, 88.5, 5.5, 0.0, 180.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 39: array([8.5, 26.5, 314.5, 62.5, 61.5, 148.5, 346.0, 7.5, 4.5, 198.5, 1.5, 190.5, 14.5, 21.5, 181.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 40: array([41.5, 0.0, 8.5, 281.5, 139.5, 136.5, 69.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 41: array([203.5, 67.5, 196.5, 16.5, 106.5, 193.5, 61.5, 143.0, 427.0, 140.5, 30.5, 0.0, 196.5, 199.5, 209.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42: array([135.5, 191.5, 74.5, 0.0, 65.0, 10.5, 71.5, 0.0, 0.0, 64.5, 0.0, 195.5, 43.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 43: array([108.5, 650.5, 65.5, 603.0, 199.5, 205.5, 0.0, 588.0, 194.0, 202.0, 210.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 44: array([82.5, 0.0, 127.5, 35.5, 7.5, 0.0, 249.5, 128.5, 177.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 45: array([148.5, 127.5, 64.5, 81.0, 0.0, 179.5, 55.5, 0.0, 57.5, 0.0, 89.5, 328.0, 99.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 46: array([191.5, 184.5, 67.5, 0.0, 83.5, 63.5, 8.5, 0.0, 227.5, 0.0, 180.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 47: array([178.5, 0.0, 59.5, 120.0, 154.5, 0.0, 0.0, 126.5, 281.5, 0.0, 0.0, 0.0, 0.0]), 48: array([289.5, 138.5, 69.5, 124.5, 0.0, 61.5, 193.5, 126.0, 6.5, 0.0, 0.0, 153.5, 381.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 49: array([221.5, 55.5, 187.5, 97.0, 0.0, 196.5, 201.5, 472.0, 191.5, 0.0, 0.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 50: array([60.5, 78.5, 79.5, 183.0, 0.0, 19.5, 72.5, 0.0, 0.0, 305.5, 0.0, 0.0, 0.0, 0.0, 0.0]), 51: array([221.5, 55.5, 187.5, 217.5, 0.0, 196.5, 201.5, 48.5, 1.5, 0.0, 0.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 52: array([41.5, 0.0, 143.5, 289.5, 56.5, 58.5, 143.5, 0.0, 87.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 53: array([131.5, 57.5, 189.5, 53.5, 54.5, 159.5, 184.5, 130.5, 0.0, 132.5, 0.0, 0.0, 86.5, 0.0, 74.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 54: array([6.5, 151.5, 70.5, 142.0, 68.5, 69.5, 68.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 75.5, 0.0, 0.0, 0.0]), 55: array([81.5, 140.5, 76.5, 0.0, 0.0, 184.5, 11.5, 126.5, 192.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 56: array([120.5, 136.5, 58.5, 0.0, 0.0, 90.5, 173.5, 119.5, 139.5, 6.5, 9.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 57: array([82.5, 77.5, 189.5, 76.5, 8.5, 176.5, 194.5, 72.5, 192.5, 1.5, 0.0, 0.0, 4.5, 1.5, 189.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 58: array([9.5, 135.5, 9.5, 191.5, 8.5, 0.0, 0.0, 0.0, 0.0, 10.5, 0.0, 0.0, 0.0]), 59: array([82.5, 77.5, 189.5, 62.5, 8.5, 0.5, 195.5, 384.5, 0.0, 168.5, 0.0, 0.0, 0.0, 8.5, 189.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 60: array([17.5, 137.5, 134.5, 9.0, 87.5, 67.0, 149.5, 0.0, 0.0, 0.0, 0.0, 0.0, 14.5, 0.0, 68.5, 0.0, 0.0, 0.0, 0.0]), 61: array([226.5, 225.5, 3.5, 695.0, 167.5, 69.5, 106.5, 217.5, 106.5, 0.0, 0.0, 0.0, 0.0, 199.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 62: array([194.5, 72.5, 66.5, 172.5, 138.5, 0.0, 0.0, 0.0, 0.0, 0.0, 76.5, 0.0, 0.0]), 63: array([226.5, 225.5, 3.5, 695.0, 216.0, 69.5, 106.5, 217.5, 106.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 64: array([82.5, 0.0, 118.0, 0.0, 7.5, 282.0, 167.5, 0.0, 0.0, 0.0, 0.0]), 65: array([158.5, 94.5, 188.5, 9.5, 196.5, 22.5, 201.5, 199.5, 4.5, 0.0, 164.5, 219.5, 64.0, 225.0, 6.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 66: array([63.5, 143.5, 79.5, 191.5, 191.5, 327.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 67: array([158.5, 97.5, 188.5, 216.5, 164.5, 22.5, 201.5, 21.5, 0.0, 92.5, 196.5, 219.5, 64.0, 225.0, 6.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}


def __convert(cell):
    value = str(cell)
    try:
        result = int(value)
        return result
    except ValueError:
        try:
            result=float(value)
            if math.isnan(result):
                print('NaN value found. Aborting.')
                sys.exit(1)
            return result
        except ValueError:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            return result
        except Exception as e:
            print(f"An exception of type {type(e).__name__} was encountered. Aborting.")
            sys.exit(1)


def __get_key(val, dictionary):
    if dictionary == {}:
        return val
    for key, value in dictionary.items(): 
        if val == value:
            return key
    if val not in dictionary.values:
        print("Label key does not exist")
        sys.exit(1)


def __convertclassid(cell, classlist=[]):

    value = str(cell)
    
    if value == '':
        print('Empty value encountered for a class label. Aborting.')
        sys.exit(1)
    
    if mapping != {}:
        result = -1
        try:
            result = mapping[cell]
        except KeyError:
            print(f"The class label {value} does not exist in the class mapping. Aborting.")
            sys.exit(1)
        except Exception as e:
            print(f"An exception of type {type(e).__name__} was encountered. Aborting.")
            sys.exit(1)
        if result != int(result):
            print(f"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.")
            sys.exit(1)
        if str(result) not in classlist:
            classlist.append(str(result))
        return result
    
    try:
        result = float(cell)
        if str(result) not in classlist:
            classlist.append(str(result))
    except:
        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
        if result in classlist:
            result = classlist.index(result)
        else:
            classlist.append(str(result))
            result = classlist.index(result)
        if result != int(result):
            print(f"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.")
            sys.exit(1)
    finally:
        if result < 0:
            print(f"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.")
            sys.exit(1)

    return result


def __clean(filename, outfile, headerless=False, testfile=False, trim=False):
    classlist = []
    outbuf = []
    remove_bad_chars = lambda x: x.replace('"', '').replace(',', '').replace('(', '').replace(')', '')
    
    with open(filename, encoding='utf-8') as csv_file, open(outfile, "w+", encoding='utf-8') as f:
        
        reader = csv.reader(csv_file)
        if not headerless:
            next(reader, None)
        
        for i, row in enumerate(reader):

            if row == []:
                continue

            
            expected_row_length = len(important_idxs)
            if not trim:
                expected_row_length += len(ignorecolumns)
            if not testfile:
                expected_row_length += 1
            actual_row_length = len(row)

            if testfile and actual_row_length == expected_row_length + 1:
                error_str = f"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. "
                error_str += f"Please check that the CSV contains no target column otherwise use -validate. Aborting."
                print(error_str)
                sys.exit(1)
            
            if actual_row_length != expected_row_length:
                print(f"We found {actual_row_length} columns but expected {expected_row_length} columns.")
                sys.exit(1)            

            if testfile:
                if len(row) == 1:
                    converted_row = [str(__convert(remove_bad_chars(row[0])))]
                else:
                    converted_row = [str(__convert(remove_bad_chars(element))) + "," for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         
            else:
                converted_row = [str(__convert(remove_bad_chars(element))) + "," for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]
            outbuf.extend(converted_row)

            if len(outbuf) < IOBUF:
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        
        print(''.join(outbuf), end="", file=f)

    n_classes_found = len(classlist)
    if not testfile and n_classes_found < 2:
        print(f"Only {n_classes_found} classes were found. Aborting.")
        sys.exit(1)


def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):
    stats = {}
    if labels is None:
        labels = np.array(list(set(list(y_true.astype('int')))))
    else:
        labels = np.asarray(labels)
        if np.all([l not in y_true for l in labels]):
            raise ValueError("At least one label specified must be in y_true")
    n_labels = labels.size

    for class_i in range(n_labels):
        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}
        class_i_indices = np.argwhere(y_true==class_i)
        not_class_i_indices = np.argwhere(y_true!=class_i)
        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))
        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))
        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))
        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))

    if not report_cmat:
        if json:
            return np.array([]), stats
        else:
            sys.exit(0)

    if sample_weight is None:
        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    else:
        sample_weight = np.asarray(sample_weight)
    if y_true.shape[0]!=y_pred.shape[0]:
        raise ValueError("y_true and y_pred must be of the same length")

    if normalize not in ['true', 'pred', 'all', None]:
        raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


    label_to_ind = {y: x for x, y in enumerate(labels)}
    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
    y_pred = y_pred[ind]
    y_true = y_true[ind]

    sample_weight = sample_weight[ind]
    if sample_weight.dtype.kind in {'i', 'u', 'b'}:
        dtype = np.int64
    else:
        dtype = np.float64
    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()

    with np.errstate(all='ignore'):
        if normalize == 'true':
            cm = cm / cm.sum(axis=1, keepdims=True)
        elif normalize == 'pred':
            cm = cm / cm.sum(axis=0, keepdims=True)
        elif normalize == 'all':
            cm = cm / cm.sum()
        cm = np.nan_to_num(cm)
    return cm, stats


def __predict(arr, headerless, csvfile, trim=False):
    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        reader = csv.reader(csvinput)
        if not headerless:
            if trim:
                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])
            else:
                header = ','.join(next(reader, None) + ['Prediction'])
            print(header)
        outputs = __classify(arr)
        for i, row in enumerate(reader):
            pred = str(__get_key(int(outputs[i]), mapping))
            if trim:
                row = ['"' + field + '"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]
            else:
                row = ['"' + field + '"' if ',' in field else field for field in row]            
            row.append(pred)
            print(','.join(row))


def __preprocess_and_clean_in_memory(arr):
    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):
        print(f'The input to \'predict\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')
        sys.exit(1)
    clean_arr = np.zeros((len(arr), len(important_idxs)))
    for i, row in enumerate(arr):
        try:
            row_used_cols_only = [row[i] for i in important_idxs]
        except IndexError:
            error_str = f"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr})."
            if len(arr) == num_attr and len(arr[0]) != num_attr:
                error_str += "\n\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' "
                error_str += "rather than as an element of a list. Make sure that even single instances "
                error_str += "are enclosed in a list. Example: predict_in_memory(0) is invalid but "
                error_str += "predict_in_memory([0]) is valid."
            print(error_str)
            sys.exit(1)
        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]
    return clean_arr


def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):
    if xs is None:
        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])

    current_node_per_row = np.zeros(xs.shape[0]).astype('int')
    values = np.empty(xs.shape[0])
    values.fill(np.nan)

    while np.isnan(values).any():

        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)
        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)

        if row_idxs_at_leaf.shape[0] > 0:

            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)
            current_node_per_row[row_idxs_at_leaf] = -1

        if row_idxs_at_branch.shape[0] > 0:

            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')
            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')
            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)

            branch_nodes = current_node_per_row[row_idxs_at_branch]
            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,
                                                                right_children[branch_nodes].astype('int'),
                                                                (right_children[branch_nodes] + 1).astype('int'))

    return values


def __build_logit_func(n_trees, clss):

    def __logit_func(xs, serial, data_shape, pool=None):
        if serial:
            sum_of_leaf_values = np.zeros(xs.shape[0])
            for booster_index in range(clss, n_trees, n_classes):
                sum_of_leaf_values += __evaluate_tree(xs, split_vals_dict[booster_index], split_feats_dict[booster_index],
                                                right_children_dict[booster_index], logits_dict[booster_index])
        else:
            sum_of_leaf_values = np.sum(list(pool.starmap(__evaluate_tree,
                                            [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],
                                              right_children_dict[booster_index], logits_dict[booster_index])
                                    for booster_index in range(clss, n_trees, n_classes)])), axis=0)
        return sum_of_leaf_values

    return __logit_func


def __init_worker(X, X_shape):
    var_dict['X'] = X
    var_dict['X_shape'] = X_shape


def __classify(rows, return_probabilities=False, force_serial=False):
    if force_serial:
        serial = True
    else:
        serial = default_to_serial
    if isinstance(rows, list):
        rows = np.array(rows)

    logits = [__build_logit_func(68, clss) for clss in range(n_classes)]

    if serial:
        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T
    else:
        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])
        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)
        np.copyto(shared_arr_np, rows)

        procs = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))
        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T

    if return_probabilities:
        exps = np.exp(o)
        Z = np.sum(exps, axis=1).reshape(-1, 1)
        output = exps/Z 
    else:
        output = np.argmax(o,axis=1)
    return output


def __validate_kwargs(kwargs):
    for key in kwargs:
        if key not in ['return_probabilities', 'force_serial']:
        
            print(f'{key} is not a keyword argument for Brainome\'s {classifier_type} predictor. Please see the documentation.')
            sys.exit(1)


def predict(arr, remap=True, **kwargs):
    """
    Parameters
    ----------
    arr : list[list]
        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.

    remap : bool
        If True and 'return_probs' is False, remaps the output to the original class
        label. If 'return_probs' is True this instead adds a header indicating which
        original class label each column of output corresponds to.
    
    **kwargs :
        return_probabilities : bool
            If true, return class membership probabilities instead of classifications.
        force_serial : bool
            If true, model inference is done in serial rather than in parallel. This is
            useful if calling "predict" repeatedly inside a for-loop.
        
    Returns
    -------
    output : np.ndarray
        A numpy array of

            1. Class predictions if 'return_probabilities' is False.
            2. Class probabilities if 'return_probabilities' is True.
        """
    kwargs = kwargs or {}
    __validate_kwargs(kwargs)
    remove_bad_chars = lambda x: str(x).replace('"', '').replace(',', '').replace('(', '').replace(')', '')
    arr = [[remove_bad_chars(field) for field in row] for row in arr]
    arr = __preprocess_and_clean_in_memory(arr)
    output = __classify(arr, **kwargs)
    if remap:
        if len(output.shape) > 1: # probabilities were returned
            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)
            output = np.concatenate((header, output), axis=0)
        else:
            output = np.array([__get_key(prediction, mapping) for prediction in output])
    return output


def validate(cleanarr):
    """
    Parameters
    ----------
    cleanarr : np.ndarray
        An array of float values that has undergone each pre-
        prediction step.

    Returns
    -------
    count : int
        A count of the number of instances in cleanarr.

    correct_count : int
        A count of the number of correctly classified instances in
        cleanarr.

    numeachclass : dict
        A dictionary mapping each class to its number of instances.

    outputs : np.ndarray
        The output of the predictor's '__classify' method on cleanarr.
    """
    outputs = __classify(cleanarr[:, :-1])
    count, correct_count = 0, 0
    numeachclass = {}
    for k, o in enumerate(outputs):
        if int(o) == int(float(cleanarr[k, -1])):
            correct_count += 1
        if int(float(cleanarr[k, -1])) in numeachclass.keys():
            numeachclass[int(float(cleanarr[k, -1]))] += 1
        else:
            numeachclass[int(float(cleanarr[k, -1]))] = 1
        count += 1
    return count, correct_count, numeachclass, outputs
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    if args.validate:
        args.trim = True

    is_testfile = not args.validate
    
    cleanfile = tempfile.NamedTemporaryFile().name
    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')
    if len(cleanarr.shape) == 1:
        if args.trim and len(important_idxs) == 1:
            cleanarr = cleanarr.reshape(-1, 1)
        elif len(open(cleanfile, 'r').read().splitlines()) == 1:
            cleanarr = cleanarr.reshape(1, -1)

    if not args.trim and ignorecolumns != []:
        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))

    if not args.validate:
        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)
    else:
        count, correct_count, numeachclass, preds = validate(cleanarr)
        
        true_labels = cleanarr[:, -1]
        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)
        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()
        best_guess = round(100.0 * np.max(classbalance), 2)
        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))
        modelacc = int(float(correct_count * 10000) / count) / 100.0

        if args.json:
            json_dict = {'instance_count': count,
                         'classifier_type': classifier_type,
                         'classes': n_classes,
                         'number_correct': correct_count,
                         'accuracy': {
                             'best_guess': best_guess,
                             'improvement': modelacc - best_guess,
                             'model_accuracy': modelacc,
                         },
                         'model_capacity': model_cap,
                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,
                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,
                         'shannon_entropy_of_labels': H,
                         'classbalance': classbalance}
            
        else:
            print("Classifier Type:                    Random Forest")
            print(f"System Type:                        {n_classes}-way classifier")
            print()
            print("Accuracy:")
            print("    Best-guess accuracy:            {:.2f}%".format(best_guess))
            print("    Model accuracy:                 {:.2f}%".format(modelacc) + " (" + str(int(correct_count)) + "/" + str(count) + " correct)")
            print("    Improvement over best guess:    {:.2f}%".format(modelacc - best_guess) + " (of possible " + str(round(100 - best_guess, 2)) + "%)")
            print()
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            if classifier_type == '\'NN\'':
                print("Model Capacity Utilized:            {:.0f} bits".format(cap_utilized))  # noqa
            print("Generalization ratio:               {:.2f}".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + " bits/bit")

        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)

        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            json_dict['multiclass_stats'] = stats
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx.astype('str')
            labels = np.array(list(mapping.keys())).reshape(-1, 1)
            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')
            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0
            max_class_name_len = len('target') + 2
            for classs in mapping.keys():
                max_class_name_len = max(max_class_name_len, len(classs))
            for key in stats.keys():
                class_stats = stats[key]
                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(
                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))
            print()
            print("Confusion Matrix:")
            print()
            max_len_value = int(np.max(np.vectorize(len)(mtrx)))
            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value

            print(" " * 4 + "{:>{}} |{:^{}}".format("Actual", max_class_name_len, "Predicted", max_pred_len))
            print(" " * 4 + "-" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))
            for row in mtrx:
                print(str(" " * 4 + "{:>{}}".format(row[0], max_class_name_len)) + " |" + "{:^{}}".format(
                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))
            print()
            print("Accuracy by Class:")
            print()
            print(" " * 4 + "{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}".format('target',
                                                                                                              max_class_name_len,
                                                                                                              'TP', max_TP_len,
                                                                                                              'FP', max_FP_len,
                                                                                                              'TN', max_TN_len,
                                                                                                              'FN', max_FN_len,
                                                                                                              'TPR', 'TNR',
                                                                                                              'PPV', 'NPV',
                                                                                                              'F1', 'TS'))
            print(" " * 4 + "-" * max_class_name_len + ' | ' + "-" * (
                max_TP_len) + ' ' + "-" * max_FP_len + ' ' + "-" * max_TN_len + ' ' + "-" * max_FN_len + (' ' + 7 * "-") * 6)
            for raw_class in mapping.keys():
                class_stats = stats[int(mapping[raw_class])]
                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(
                    class_stats['TP'] + class_stats['FN']) != 0 else 0
                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(
                    class_stats['TN'] + class_stats['FP']) != 0 else 0
                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(
                    class_stats['TP'] + class_stats['FP']) != 0 else 0
                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(
                    class_stats['TN'] + class_stats['FN']) != 0 else 0
                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(
                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0
                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(
                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0
                print(" " * 4 + "{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}".format(raw_class,
                                                                                                                  max_class_name_len,
                                                                                                                  class_stats['TP'],
                                                                                                                  max_TP_len,
                                                                                                                  class_stats['FP'],
                                                                                                                  max_FP_len,
                                                                                                                  class_stats['TN'],
                                                                                                                  max_TN_len,
                                                                                                                  class_stats['FN'],
                                                                                                                  max_FN_len,
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * TPR, 2)),
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * TNR, 2)),
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * PPV, 2)),
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * NPV, 2)),
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * F1, 2)),
                                                                                                                  "{:0.2f}%".format(
                                                                                                                      round(100.0 * TS, 2))))
            
    os.remove(cleanfile)
    
